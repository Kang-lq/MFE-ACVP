import os
import torch
import numpy as np
import pandas as pd
import pickle
import joblib
from tqdm import tqdm
from collections import defaultdict
from sklearn.preprocessing import OneHotEncoder
# è®¾ç½®ä½¿ç”¨ç¬¬ 2 ä¸ª GPUï¼ˆç´¢å¼•ä» 0 å¼€å§‹ï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "3"

# ç„¶ååœ¨æ‚¨çš„è®¾å¤‡é€‰æ‹©ä»£ç ä¸­ä¿æŒåŸæ ·
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"å½“å‰ä½¿ç”¨çš„è®¾å¤‡: {device}")
class SecondaryStructureFeatureExtractor:
    def __init__(self):
        # å®šä¹‰æ•°æ®é›†è·¯å¾„ï¼ˆæ›´æ–°ä¸ºæ–°çš„è·¯å¾„ç»“æ„ï¼‰
        self.datasets = {
            'train_pos': '/home/kanglq/code_file/MyProject/Features_model/Data/CSV/train_pos',
            'train_neg': '/home/kanglq/code_file/MyProject/Features_model/Data/CSV/train_neg',
            'test_pos': '/home/kanglq/code_file/MyProject/Features_model/Data/CSV/test_pos',
            'test_neg': '/home/kanglq/code_file/MyProject/Features_model/Data/CSV/test_neg',
            'independent_pos': '/home/kanglq/code_file/MyProject/Features_model/Data/CSV/independent_pos',
            'independent_neg': '/home/kanglq/code_file/MyProject/Features_model/Data/CSV/independent_neg'
        }
        self.output_dir = '/home/kanglq/code_file/MyProject/Features_model/Features_results/Struc_Feature/Secondary_Feature'
        os.makedirs(self.output_dir, exist_ok=True)

        # å‚æ•°é…ç½®ï¼ˆè®­ç»ƒé˜¶æ®µç¡®å®šçš„å‚æ•°ï¼‰
        self.confidence_threshold = None  # å°†åœ¨fitä¸­ç¡®å®š
        self.short_peptide_length = 15
        self.fitted_ = False  # æ ‡è®°æ˜¯å¦å·²å®Œæˆæ‹Ÿåˆ

        # å›ºå®šå‚æ•°ï¼ˆä¸éœ€è¦æ‹Ÿåˆï¼‰
        self.kd_scale = {
            'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,
            'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,
            'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,
            'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2
        }

        # æ–°å¢ï¼šä½ç½®ç‰¹å¾ç¼–ç å™¨
        self.position_encoder = OneHotEncoder(
            categories=[['none', 'N-term', 'middle', 'C-term']],
            handle_unknown='ignore'
        )

    def _determine_threshold(self, train_dfs):
        """åŸºäºè®­ç»ƒæ•°æ®ç¡®å®šæœ€ä½³ç½®ä¿¡åº¦é˜ˆå€¼"""
        all_confidences = []
        for df in train_dfs:
            conf = df['p[q3_H]'] + df['p[q3_E]'] + df['p[q3_C]']
            all_confidences.extend(conf.values)

        # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„ä¸­ä½æ•°ä½œä¸ºé˜ˆå€¼
        self.confidence_threshold = np.median(all_confidences)
        print(f"âœ… ç¡®å®šç½®ä¿¡åº¦é˜ˆå€¼ä¸º: {self.confidence_threshold:.3f} (åŸºäºè®­ç»ƒæ•°æ®)")

    def _adjust_threshold(self, seq_length):
        """åŠ¨æ€è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆåŸºäºè®­ç»ƒç¡®å®šçš„é˜ˆå€¼ï¼‰"""
        if not self.fitted_:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨fit()æ–¹æ³•è®­ç»ƒç‰¹å¾æå–å™¨ï¼")

        if seq_length < self.short_peptide_length:
            return max(0.5, self.confidence_threshold - 0.02 * (self.short_peptide_length - seq_length))
        return self.confidence_threshold

    def fit(self):
        """ä»…åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆå‚æ•°"""
        if self.fitted_:
            print("âš ï¸ ç‰¹å¾æå–å™¨å·²æ‹Ÿåˆï¼Œè·³è¿‡é‡å¤æ‹Ÿåˆ")
            return

        print("=" * 50)
        print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒé˜¶æ®µï¼ˆä»…ä½¿ç”¨train_poså’Œtrain_negï¼‰")

        # åŠ è½½æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼ˆä¸¥æ ¼åªä½¿ç”¨è®­ç»ƒé›†ï¼‰
        train_dfs = []
        for dataset in ['train_pos', 'train_neg']:
            input_dir = self.datasets[dataset]
            for filename in os.listdir(input_dir):
                if filename.endswith('.csv'):
                    df = self._load_csv(os.path.join(input_dir, filename))
                    if df is not None:
                        train_dfs.append(df)

        # ç¡®å®šå…³é”®å‚æ•°
        self._determine_threshold(train_dfs)

        # æ–°å¢ï¼šæ‹Ÿåˆä½ç½®ç‰¹å¾ç¼–ç å™¨
        dummy_positions = [['N-term'], ['middle'], ['C-term'], ['none']]
        self.position_encoder.fit(dummy_positions)

        self.fitted_ = True

        # ä¿å­˜ç‰¹å¾æå–å™¨
        self._save_extractor()
        print("=" * 50 + "\n")

    def _load_csv(self, csv_path):
        """åŠ è½½å¹¶éªŒè¯CSVæ–‡ä»¶"""
        try:
            df = pd.read_csv(csv_path, sep=',', header=0)
            df.columns = df.columns.str.strip()

            required_columns = ['seq', 'q3', 'p[q3_H]', 'p[q3_E]', 'p[q3_C]']
            if not all(col in df.columns for col in required_columns):
                print(f"âš ï¸ æ–‡ä»¶ {os.path.basename(csv_path)} ç¼ºå°‘å¿…è¦åˆ—ï¼Œå·²è·³è¿‡")
                return None

            return df
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ–‡ä»¶ {os.path.basename(csv_path)} å‡ºé”™: {str(e)}")
            return None

    def _calculate_global_stats(self, df):
        """è®¡ç®—å…¨å±€ç»Ÿè®¡ç‰¹å¾"""
        threshold = self._adjust_threshold(len(df))
        valid_df = df[(df['p[q3_H]'] + df['p[q3_E]'] + df['p[q3_C]']) >= threshold]

        if len(valid_df) == 0:
            return {
                'H_percent': 0.0, 'E_percent': 0.0, 'C_percent': 0.0,
                'dominant_strength': 0.0
            }

        h_percent = (valid_df['q3'] == 'H').mean()
        e_percent = (valid_df['q3'] == 'E').mean()
        c_percent = (valid_df['q3'] == 'C').mean()

        dominant_strength = max(
            valid_df['p[q3_H]'].mean(),
            valid_df['p[q3_E]'].mean(),
            valid_df['p[q3_C]'].mean()
        )

        return {
            'H_percent': float(h_percent),
            'E_percent': float(e_percent),
            'C_percent': float(c_percent),
            'dominant_strength': float(dominant_strength)
        }

    def _calculate_fragment_features(self, df):
        """è®¡ç®—ç‰‡æ®µåˆ†å¸ƒç‰¹å¾"""
        ss_seq = df['q3'].values
        max_h = current_h = max_e = current_e = 0
        h_starts = []
        e_starts = []

        for i, ss in enumerate(ss_seq):
            if ss == 'H':
                current_h += 1
                current_e = 0
                if current_h == 1:
                    h_starts.append(i)
                max_h = max(max_h, current_h)
            elif ss == 'E':
                current_e += 1
                current_h = 0
                if current_e == 1:
                    e_starts.append(i)
                max_e = max(max_e, current_e)
            else:
                current_h = current_e = 0

        def _position_feature(starts, length):
            if not starts:
                return {'pos': 'none', 'ratio': 0.0}
            pos = starts[0] / length
            if pos < 0.33:
                return {'pos': 'N-term', 'ratio': float(pos)}
            elif pos > 0.66:
                return {'pos': 'C-term', 'ratio': float(pos)}
            else:
                return {'pos': 'middle', 'ratio': float(pos)}

        h_pos = _position_feature(h_starts, len(df))
        e_pos = _position_feature(e_starts, len(df))

        def _calc_cv(ss_type):
            lengths = []
            current = 0
            for ss in ss_seq:
                if ss == ss_type:
                    current += 1
                elif current > 0:
                    lengths.append(current)
                    current = 0
            if current > 0:
                lengths.append(current)
            return float(np.std(lengths) / np.mean(lengths)) if lengths else 0.0

        return {
            'max_H_length': float(max_h),
            'max_E_length': float(max_e),
            'H_position': h_pos['pos'],
            'E_position': e_pos['pos'],
            'H_position_ratio': float(h_pos['ratio']),
            'E_position_ratio': float(e_pos['ratio']),
            'H_length_CV': _calc_cv('H'),
            'E_length_CV': _calc_cv('E')
        }

    def _calculate_transition_matrix(self, df):
        """è®¡ç®—é©¬å°”å¯å¤«è½¬ç§»çŸ©é˜µ"""
        ss_types = ['H', 'E', 'C']
        trans_counts = np.zeros((3, 3))

        for i in range(len(df) - 1):
            current = df.iloc[i]['q3']
            next_ = df.iloc[i + 1]['q3']
            if current in ss_types and next_ in ss_types:
                trans_counts[ss_types.index(current), ss_types.index(next_)] += 1

        row_sums = trans_counts.sum(axis=1, keepdims=True)
        trans_matrix = np.divide(
            trans_counts,
            row_sums,
            out=np.zeros_like(trans_counts),
            where=row_sums != 0
        )

        try:
            eigenvalues, eigenvectors = np.linalg.eig(trans_matrix.T)
            stationary = eigenvectors[:, np.isclose(eigenvalues, 1)].real[:, 0]
            stationary /= stationary.sum()
        except:
            stationary = np.array([1 / 3, 1 / 3, 1 / 3])

        features = {}
        for i, from_ss in enumerate(ss_types):
            for j, to_ss in enumerate(ss_types):
                features[f'trans_{from_ss}_to_{to_ss}'] = float(trans_matrix[i, j])
            features[f'stat_{from_ss}'] = float(stationary[i])

        return features

    def _calculate_physicochemical(self, df):
        """è®¡ç®—ç‰©ç†åŒ–å­¦ç‰¹å¾"""
        helix_hydro = []
        current_seq = []
        for _, row in df.iterrows():
            if row['q3'] == 'H':
                current_seq.append(row['seq'])
            elif current_seq:
                hydro = np.mean([self.kd_scale.get(aa, 0) for aa in current_seq])
                helix_hydro.append(float(hydro))
                current_seq = []
        if current_seq:
            hydro = np.mean([self.kd_scale.get(aa, 0) for aa in current_seq])
            helix_hydro.append(float(hydro))

        polar_aas = {'D', 'E', 'H', 'K', 'N', 'Q', 'R', 'S', 'T', 'Y'}
        polar_pos = [i for i, row in df.iterrows()
                     if row['q3'] == 'E' and row['seq'] in polar_aas]
        if len(polar_pos) > 1:
            polar_clustering = float(1 / np.mean(np.diff(polar_pos)))
        else:
            polar_clustering = 0.0

        return {
            'avg_helix_hydro': float(np.mean(helix_hydro)) if helix_hydro else 0.0,
            'polar_clustering_sheet': float(polar_clustering)
        }

    def _encode_features(self, features):
        """ç¼–ç ç‰¹å¾ä¸ºæ•°å€¼å‹æ•°ç»„"""
        # æå–ä½ç½®ç‰¹å¾å¹¶ç¼–ç 
        h_position = features.pop('H_position')
        e_position = features.pop('E_position')

        # ç¼–ç ä½ç½®ç‰¹å¾
        h_pos_encoded = self.position_encoder.transform([[h_position]]).toarray()[0]
        e_pos_encoded = self.position_encoder.transform([[e_position]]).toarray()[0]

        # åˆ›å»ºæ•°å€¼ç‰¹å¾å­—å…¸
        numeric_features = {}
        for k, v in features.items():
            if isinstance(v, (int, float, np.number)):
                numeric_features[k] = float(v)
            elif isinstance(v, str):
                numeric_features[k] = 0.0  # å¤„ç†æ„å¤–å­—ç¬¦ä¸²
            else:
                numeric_features[k] = float(v) if v is not None else 0.0

        # æ·»åŠ ç¼–ç åçš„ä½ç½®ç‰¹å¾
        for i, val in enumerate(h_pos_encoded):
            numeric_features[f'H_position_{i}'] = float(val)
        for i, val in enumerate(e_pos_encoded):
            numeric_features[f'E_position_{i}'] = float(val)

        return numeric_features

    def extract_features(self, csv_path):
        """ä»å•ä¸ªCSVæ–‡ä»¶æå–ç‰¹å¾å¹¶ç¡®ä¿æ•°å€¼ç±»å‹"""
        if not self.fitted_:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨fit()æ–¹æ³•è®­ç»ƒç‰¹å¾æå–å™¨ï¼")

        df = self._load_csv(csv_path)
        if df is None:
            return None

        # è®¡ç®—åŸå§‹ç‰¹å¾
        raw_features = {}
        raw_features.update(self._calculate_global_stats(df))
        raw_features.update(self._calculate_fragment_features(df))  # ç¡®ä¿è°ƒç”¨è¯¥æ–¹æ³•
        raw_features.update(self._calculate_transition_matrix(df))
        raw_features.update(self._calculate_physicochemical(df))

        # ç¼–ç ä¸ºæ•°å€¼å‹ç‰¹å¾
        numeric_features = self._encode_features(raw_features)
        return numeric_features

    def process_dataset(self, dataset_name):
        """å¤„ç†æ•´ä¸ªæ•°æ®é›†å¹¶ä¿å­˜ä¸ºæ•°å€¼æ•°ç»„"""
        if not self.fitted_ and dataset_name.startswith(('test', 'independent')):
            raise RuntimeError("è¯·å…ˆè°ƒç”¨fit()æ–¹æ³•è®­ç»ƒç‰¹å¾æå–å™¨ï¼")

        input_dir = self.datasets[dataset_name]
        features = []
        labels = []

        for filename in tqdm(os.listdir(input_dir),
                             desc=f"ğŸ“Š å¤„ç† {dataset_name}"):
            if filename.endswith('.csv'):
                csv_path = os.path.join(input_dir, filename)
                peptide_id = filename.split('.')[0]
                feat = self.extract_features(csv_path)
                if feat is not None:
                    features.append(list(feat.values()))
                    labels.append(1 if 'pos' in dataset_name else 0)

        feature_array = np.array(features, dtype=np.float32)
        label_array = np.array(labels, dtype=np.int32)

        # ä¿å­˜æ•°æ®
        feature_path = os.path.join(self.output_dir, f"{dataset_name}_features.npy")
        label_path = os.path.join(self.output_dir, f"{dataset_name}_labels.npy")
        np.save(feature_path, feature_array)
        np.save(label_path, label_array)

        print(f"ğŸ’¾ ä¿å­˜ {dataset_name} ç‰¹å¾åˆ° {feature_path}")
        print(f"  - ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {feature_array.shape}")
        print(f"  - æ ‡ç­¾çŸ©é˜µå½¢çŠ¶: {label_array.shape}")

    def _save_extractor(self):
        """ä¿å­˜ç‰¹å¾æå–å™¨"""
        extractor_path = os.path.join(self.output_dir, "feature_extractor.pkl")
        joblib.dump(self, extractor_path)
        print(f"ğŸ’¾ ä¿å­˜ç‰¹å¾æå–å™¨åˆ° {extractor_path}")

    @classmethod
    def load_extractor(cls, path):
        """åŠ è½½ç‰¹å¾æå–å™¨"""
        return joblib.load(path)

    def process_all_datasets(self):
        """å®Œæ•´çš„ç‰¹å¾æå–æµç¨‹"""
        print("=" * 50)
        print("ğŸš€ å¼€å§‹ç‰¹å¾æå–æµç¨‹")

        # 1. è®­ç»ƒé˜¶æ®µï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰
        self.fit()

        # 2. å¤„ç†æ‰€æœ‰æ•°æ®é›†ï¼ˆåŒ…æ‹¬ç‹¬ç«‹æ•°æ®é›†ï¼‰
        for dataset in self.datasets:
            self.process_dataset(dataset)

        print("=" * 50)
        print("ğŸ‰ ç‰¹å¾æå–å®Œæˆï¼")


if __name__ == "__main__":
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    extractor = SecondaryStructureFeatureExtractor()

    # æ‰§è¡Œå®Œæ•´æµç¨‹
    extractor.process_all_datasets()