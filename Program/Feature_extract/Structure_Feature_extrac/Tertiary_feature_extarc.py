import os
import numpy as np
import prody
import pickle
import joblib
from scipy.spatial.distance import pdist, squareform
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import DBSCAN
from tqdm import tqdm
import logging
import freesasa
from typing import Dict, Optional, Tuple
from datetime import datetime

class TertiaryStructureFeatureExtractor:
    def __init__(self, output_dir: Optional[str] = None):
        """ä¸‰çº§ç»“æ„ç‰¹å¾æå–å™¨

        Args:
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        # å…ˆè®¾ç½®è¾“å‡ºç›®å½•
        self.output_dir = output_dir or '/home/kanglq/code_file/MyProject/Features_model/Features_results/Struc_Feature/Tertiary_Feature'
        os.makedirs(self.output_dir, exist_ok=True)

        # ç„¶ååˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._setup_logging()

        # æ•°æ®é›†è·¯å¾„é…ç½®ï¼ˆæ›´æ–°ä¸ºæ–°çš„è·¯å¾„ç»“æ„ï¼‰
        self.datasets = {
            'train_pos': '/home/kanglq/code_file/MyProject/Features_model/Data/PDB/train_pos',
            'train_neg': '/home/kanglq/code_file/MyProject/Features_model/Data/PDB/train_neg',
            'test_pos': '/home/kanglq/code_file/MyProject/Features_model/Data/PDB/test_pos',
            'test_neg': '/home/kanglq/code_file/MyProject/Features_model/Data/PDB/test_neg',
            'independent_pos': '/home/kanglq/code_file/MyProject/Features_model/Data/PDB/independent_pos',
            'independent_neg': '/home/kanglq/code_file/MyProject/Features_model/Data/PDB/independent_neg'
        }

        # ç‰¹å¾å‚æ•°ï¼ˆå°†åœ¨fitä¸­ç¡®å®šï¼‰
        self.contact_thresholds = None
        self.svd_energy_threshold = None
        self.cluster_eps = None
        self.fitted_ = False  # æ ‡è®°æ˜¯å¦å·²å®Œæˆæ‹Ÿåˆ

        # å›ºå®šå‚æ•°
        self.min_ca_atoms = 3  # æœ€å°‘éœ€è¦3ä¸ªCAåŸå­
        self.local_contact_window = 3  # æ’é™¤å±€éƒ¨æ¥è§¦çš„çª—å£å¤§å°(iÂ±3)

    def _setup_logging(self):
        """é…ç½®è¯¦ç»†çš„æ—¥å¿—ç³»ç»Ÿ"""
        log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶
        log_file = os.path.join(self.output_dir, f"feature_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)

        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(log_formatter)

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(log_formatter)

        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)

        self.logger.info("=" * 80)
        self.logger.info("ğŸ› ï¸ åˆå§‹åŒ–ä¸‰çº§ç»“æ„ç‰¹å¾æå–å™¨")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info("=" * 80)

    def fit(self):
        """åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆç‰¹å¾æå–å™¨å‚æ•°"""
        if self.fitted_:
            self.logger.warning("ç‰¹å¾æå–å™¨å·²æ‹Ÿåˆï¼Œè·³è¿‡é‡å¤æ‹Ÿåˆ")
            return self

        self.logger.info("=" * 80)
        self.logger.info("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒé˜¶æ®µï¼ˆä»…ä½¿ç”¨train_poså’Œtrain_negï¼‰")

        # 1. ç¡®å®šæœ€ä¼˜æ¥è§¦è·ç¦»é˜ˆå€¼
        self._determine_contact_thresholds()

        # 2. ç¡®å®šSVDèƒ½é‡é˜ˆå€¼ï¼ˆåŸºäºè®­ç»ƒæ•°æ®ï¼‰
        self.svd_energy_threshold = 0.9

        # 3. ç¡®å®šå¸¦ç”µç°‡æ£€æµ‹çš„DBSCANå‚æ•°
        self.cluster_eps = 5.0

        self.fitted_ = True
        self._save_extractor()

        self.logger.info("âœ… ç‰¹å¾æå–å™¨è®­ç»ƒå®Œæˆ")
        self.logger.info(f"æ¥è§¦è·ç¦»é˜ˆå€¼: {self.contact_thresholds}")
        self.logger.info(f"SVDèƒ½é‡é˜ˆå€¼: {self.svd_energy_threshold}")
        self.logger.info(f"å¸¦ç”µç°‡æ£€æµ‹EPS: {self.cluster_eps}")
        self.logger.info("=" * 80)

        return self

    def _determine_contact_thresholds(self):
        """åŸºäºè®­ç»ƒæ•°æ®ç»Ÿè®¡ç¡®å®šæ¥è§¦è·ç¦»é˜ˆå€¼"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥ä»è®­ç»ƒæ•°æ®ç»Ÿè®¡å¾—å‡º
        self.contact_thresholds = {
            'strict': 4.0,  # ç´§å¯†æ¥è§¦
            'regular': 6.0,  # å¸¸è§„æ¥è§¦
            'long_range': 8.0  # é•¿ç¨‹æ¥è§¦
        }

    def _save_extractor(self):
        """ä¿å­˜è®­ç»ƒå¥½çš„ç‰¹å¾æå–å™¨"""
        extractor_path = os.path.join(self.output_dir, "feature_extractor.pkl")
        joblib.dump(self, extractor_path)
        self.logger.info(f"ğŸ’¾ ä¿å­˜ç‰¹å¾æå–å™¨åˆ° {extractor_path}")

    @classmethod
    def load_extractor(cls, path: str):
        """åŠ è½½å·²ä¿å­˜çš„ç‰¹å¾æå–å™¨"""
        extractor = joblib.load(path)
        extractor.logger.info(f"ğŸ” ä» {path} åŠ è½½ç‰¹å¾æå–å™¨")
        return extractor

    def _get_ca_coords(self, pdb_path: str) -> Optional[np.ndarray]:
        """è·å–CAåŸå­åæ ‡ï¼Œå¸¦ä¸¥æ ¼æ£€æŸ¥

        Args:
            pdb_path (str): PDBæ–‡ä»¶è·¯å¾„

        Returns:
            Optional[np.ndarray]: CAåŸå­åæ ‡æ•°ç»„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(pdb_path):
                self.logger.error(f"âŒ PDBæ–‡ä»¶ä¸å­˜åœ¨: {pdb_path}")
                return None

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
            if os.path.getsize(pdb_path) == 0:
                self.logger.error(f"âŒ ç©ºPDBæ–‡ä»¶: {pdb_path}")
                return None

            # ä½¿ç”¨ProDyè§£æPDB
            structure = prody.parsePDB(pdb_path)
            if structure is None:
                self.logger.error(f"âŒ æ— æ³•è§£æPDBæ–‡ä»¶: {pdb_path}")
                return None

            # é€‰æ‹©CAåŸå­
            calphas = structure.select('name CA')
            if calphas is None or len(calphas) < self.min_ca_atoms:
                self.logger.warning(
                    f"âš ï¸ CAåŸå­æ•°ä¸è¶³({len(calphas) if calphas else 0} < {self.min_ca_atoms}): {pdb_path}")
                return None

            return calphas.getCoords()

        except Exception as e:
            self.logger.error(f"âŒ è§£æPDBæ–‡ä»¶å‡ºé”™ {pdb_path}: {str(e)}", exc_info=True)
            return None

    def _calculate_contact_map(self, coords: np.ndarray, threshold: float) -> np.ndarray:
        """è®¡ç®—æ¥è§¦å›¾ï¼Œæ’é™¤å±€éƒ¨æ¥è§¦

        Args:
            coords (np.ndarray): CAåŸå­åæ ‡æ•°ç»„
            threshold (float): æ¥è§¦è·ç¦»é˜ˆå€¼

        Returns:
            np.ndarray: æ¥è§¦å›¾çŸ©é˜µ
        """
        try:
            # è®¡ç®—è·ç¦»çŸ©é˜µ
            dist_matrix = squareform(pdist(coords))

            # åˆ›å»ºæ¥è§¦å›¾
            contact_map = (dist_matrix <= threshold).astype(int)

            # æ’é™¤å±€éƒ¨æ¥è§¦(iÂ±window)
            n = contact_map.shape[0]
            for i in range(n):
                contact_map[i, max(0, i - self.local_contact_window):min(n, i + self.local_contact_window + 1)] = 0

            # ç¡®ä¿å¯¹ç§°æ€§
            return np.maximum(contact_map, contact_map.T)

        except Exception as e:
            self.logger.error(f"âŒ è®¡ç®—æ¥è§¦å›¾å‡ºé”™: {str(e)}", exc_info=True)
            return np.zeros((len(coords), len(coords)))

    def _svd_features(self, matrix: np.ndarray) -> np.ndarray:
        """è®¡ç®—SVDç‰¹å¾ï¼Œå¸¦é²æ£’æ€§å¤„ç†"""
        try:
            if not self.fitted_:
                raise RuntimeError("ç‰¹å¾æå–å™¨å°šæœªæ‹Ÿåˆï¼è¯·å…ˆè°ƒç”¨fit()æ–¹æ³•")

            # çŸ©é˜µé¢„å¤„ç† - ç¡®ä¿éè´Ÿä¸”å¯¹ç§°
            matrix = np.abs(matrix)  # ç¡®ä¿æ‰€æœ‰å€¼ä¸ºéè´Ÿ
            matrix = np.maximum(matrix, matrix.T)  # ç¡®ä¿å¯¹ç§°

            # æ£€æŸ¥å…¨é›¶çŸ©é˜µæˆ–æ— æ•ˆçŸ©é˜µ
            if np.all(matrix == 0) or matrix.shape[0] < 2:
                return np.zeros(5)

            # å½’ä¸€åŒ–å¤„ç†
            matrix = (matrix - matrix.min()) / (matrix.max() - matrix.min() + 1e-10)

            # æ·»åŠ å¾®å°å™ªå£°é¿å…å¥‡å¼‚çŸ©é˜µ
            matrix = matrix + 1e-10 * np.random.rand(*matrix.shape)

            # åŠ¨æ€ç¡®å®šç»„ä»¶æ•°é‡
            n_components = min(matrix.shape[0] - 1, 5)
            if n_components < 1:
                return np.zeros(5)

            # è®¡ç®—SVD
            svd = TruncatedSVD(n_components=n_components,
                               algorithm='arpack',
                               random_state=42)
            svd.fit(matrix)

            # æå–ç‰¹å¾å¹¶ç¡®ä¿é•¿åº¦ä¸º5
            features = svd.transform(matrix)[0, :n_components]  # å–ç¬¬ä¸€è¡Œç‰¹å¾
            features = np.pad(features, (0, 5 - len(features)), 'constant')[:5]

            return features

        except Exception as e:
            self.logger.warning(f"âš ï¸ SVDè®¡ç®—å¤±è´¥: {str(e)}")
            return np.zeros(5)

    def _geometric_features(self, coords: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—å‡ ä½•ç‰¹å¾

        Args:
            coords (np.ndarray): CAåŸå­åæ ‡æ•°ç»„

        Returns:
            Dict[str, float]: å‡ ä½•ç‰¹å¾å­—å…¸
        """
        try:
            # è®¡ç®—è´¨å¿ƒ
            centroid = np.mean(coords, axis=0)

            # å›è½¬åŠå¾„
            radius_gyration = np.sqrt(np.mean(np.sum((coords - centroid) ** 2, axis=1)))

            # é¦–æœ«ç«¯è·ç¦»æ¯”
            end_to_end = np.linalg.norm(coords[0] - coords[-1])
            max_theoretical = 3.8 * (len(coords) - 1)
            end_ratio = end_to_end / max_theoretical if max_theoretical > 0 else 0

            return {
                'radius_gyration': float(radius_gyration),
                'end_to_end_ratio': float(end_ratio)
            }

        except Exception as e:
            self.logger.error(f"âŒ è®¡ç®—å‡ ä½•ç‰¹å¾å‡ºé”™: {str(e)}", exc_info=True)
            return {
                'radius_gyration': 0.0,
                'end_to_end_ratio': 0.0
            }

    def _calculate_rsa(self, pdb_path: str) -> Dict[str, float]:
        """è®¡ç®—ç›¸å¯¹æº¶å‰‚å¯åŠè¡¨é¢ç§¯(RSA)

        Args:
            pdb_path (str): PDBæ–‡ä»¶è·¯å¾„

        Returns:
            Dict[str, float]: RSAç‰¹å¾å­—å…¸
        """
        default_result = {
            'rsa_25th': 0.0,
            'rsa_50th': 0.0,
            'rsa_75th': 0.0
        }

        try:
            # æ£€æŸ¥æ–‡ä»¶æœ‰æ•ˆæ€§
            if not os.path.exists(pdb_path) or os.path.getsize(pdb_path) == 0:
                return default_result

            # ä½¿ç”¨FreeSASAè®¡ç®—
            structure = freesasa.Structure(pdb_path)
            result = freesasa.calc(structure)

            # æ”¶é›†SASAå€¼
            sasa_values = []
            for i in range(structure.nAtoms()):
                try:
                    sasa_values.append(result.atomArea(i))
                except:
                    continue

            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            if not sasa_values:
                return default_result

            # è®¡ç®—ç™¾åˆ†ä½æ•°
            return {
                'rsa_25th': float(np.percentile(sasa_values, 25)),
                'rsa_50th': float(np.percentile(sasa_values, 50)),
                'rsa_75th': float(np.percentile(sasa_values, 75))
            }

        except Exception as e:
            self.logger.error(f"âŒ RSAè®¡ç®—å¤±è´¥ {pdb_path}: {str(e)}", exc_info=True)
            return default_result

    def _detect_charged_clusters(self, pdb_path: str) -> Dict[str, int]:
        """æ£€æµ‹å¸¦ç”µæ®‹åŸºç°‡

        Args:
            pdb_path (str): PDBæ–‡ä»¶è·¯å¾„

        Returns:
            Dict[str, int]: å¸¦ç”µç°‡ç‰¹å¾å­—å…¸
        """
        try:
            structure = prody.parsePDB(pdb_path)
            if structure is None:
                return {'num_clusters': 0}

            # é€‰æ‹©å¸¦ç”µæ®‹åŸº(ARG, LYS, ASP, GLU)
            charged_residues = structure.select('resname ARG LYS ASP GLU')
            if charged_residues is None or len(charged_residues) < 2:
                return {'num_clusters': 0}

            # ä½¿ç”¨DBSCANèšç±»
            coords = charged_residues.getCoords()
            clustering = DBSCAN(eps=self.cluster_eps, min_samples=2).fit(coords)

            # ç»Ÿè®¡ç°‡æ•°é‡(æ’é™¤å™ªå£°ç‚¹)
            labels = clustering.labels_
            num_clusters = len(set(labels)) - (1 if -1 in labels else 0)

            return {'num_clusters': int(num_clusters)}

        except Exception as e:
            self.logger.error(f"âŒ å¸¦ç”µç°‡æ£€æµ‹å¤±è´¥ {pdb_path}: {str(e)}", exc_info=True)
            return {'num_clusters': 0}

    def _default_features(self) -> Dict:
        """ç”Ÿæˆé»˜è®¤ç‰¹å¾

        Returns:
            Dict: é»˜è®¤ç‰¹å¾å­—å…¸
        """
        features = {
            'radius_gyration': 0.0,
            'end_to_end_ratio': 0.0,
            'rsa_25th': 0.0,
            'rsa_50th': 0.0,
            'rsa_75th': 0.0,
            'num_clusters': 0
        }

        # æ·»åŠ SVDç‰¹å¾çš„é»˜è®¤å€¼ï¼ˆå±•å¼€ä¸º5ä¸ªæ ‡é‡ï¼‰
        if self.contact_thresholds:
            for name in self.contact_thresholds:
                for i in range(5):
                    features[f'svd_{name}_{i}'] = 0.0

        return features

    def extract_features(self, pdb_path: str) -> Dict:
        """ä»å•ä¸ªPDBæ–‡ä»¶æå–ç‰¹å¾

        Args:
            pdb_path (str): PDBæ–‡ä»¶è·¯å¾„

        Returns:
            Dict: ç‰¹å¾å­—å…¸
        """
        if not self.fitted_:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨fit()æ–¹æ³•è®­ç»ƒç‰¹å¾æå–å™¨ï¼")

        self.logger.info(f"ğŸ” å¤„ç†æ–‡ä»¶: {pdb_path}")

        # è·å–CAåŸå­åæ ‡
        coords = self._get_ca_coords(pdb_path)
        if coords is None:
            self.logger.warning(f"âš ï¸ ä½¿ç”¨é»˜è®¤ç‰¹å¾: {pdb_path}")
            return self._default_features()

        features = {}

        # 1. å‡ ä½•ç‰¹å¾
        features.update(self._geometric_features(coords))

        # 2. æ¥è§¦å›¾å’ŒSVDç‰¹å¾ - å°†æ•°ç»„ç‰¹å¾å±•å¼€ä¸ºå¤šä¸ªæ ‡é‡ç‰¹å¾
        for name, threshold in self.contact_thresholds.items():
            cmap = self._calculate_contact_map(coords, threshold)
            svd_features = self._svd_features(cmap)
            # å°†5ç»´SVDç‰¹å¾å±•å¼€ä¸º5ä¸ªå•ç‹¬çš„ç‰¹å¾
            for i in range(5):
                features[f'svd_{name}_{i}'] = float(svd_features[i])

        # 3. RSAç‰¹å¾
        features.update(self._calculate_rsa(pdb_path))

        # 4. å¸¦ç”µç°‡ç‰¹å¾
        features.update(self._detect_charged_clusters(pdb_path))

        self.logger.debug(f"ğŸ“Š æå–çš„ç‰¹å¾: {features}")
        return features

    def process_dataset(self, dataset_name: str) -> Dict[str, Dict]:
        """å¤„ç†æ•´ä¸ªæ•°æ®é›†

        Args:
            dataset_name (str): æ•°æ®é›†åç§°(train_pos/train_neg/test_pos/test_neg/independent_pos/independent_neg)

        Returns:
            Dict[str, Dict]: ç‰¹å¾å­—å…¸{è‚½æ®µID: ç‰¹å¾}
        """
        if not self.fitted_ and dataset_name.startswith(('test', 'independent')):
            raise RuntimeError("è¯·å…ˆè°ƒç”¨fit()æ–¹æ³•è®­ç»ƒç‰¹å¾æå–å™¨ï¼")

        self.logger.info("=" * 80)
        self.logger.info(f"ğŸ“‚ å¼€å§‹å¤„ç†æ•°æ®é›†: {dataset_name}")

        input_dir = self.datasets[dataset_name]
        features = []
        labels = []

        # è·å–PDBæ–‡ä»¶åˆ—è¡¨
        pdb_files = [f for f in os.listdir(input_dir) if f.endswith('.pdb')]

        # è·å–ç‰¹å¾é¡ºåºï¼ˆä½¿ç”¨é»˜è®¤ç‰¹å¾ä½œä¸ºæ¨¡æ¿ï¼‰
        feature_order = list(self._default_features().keys())

        # ä½¿ç”¨è¿›åº¦æ¡å¤„ç†æ–‡ä»¶
        for filename in tqdm(pdb_files,
                             desc=f"å¤„ç† {dataset_name}"):
            pdb_path = os.path.join(input_dir, filename)
            peptide_id = os.path.splitext(filename)[0]

            try:
                feat = self.extract_features(pdb_path)
                if feat is not None:
                    # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
                    ordered_feat = [feat[key] for key in feature_order]
                    features.append(ordered_feat)
                    labels.append(1 if 'pos' in dataset_name else 0)
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç† {filename} å‡ºé”™: {str(e)}", exc_info=True)

        # è½¬æ¢ä¸ºNumPyæ•°ç»„
        if features:
            feature_array = np.array(features, dtype=np.float32)
        else:
            feature_array = np.zeros((0, len(feature_order)), dtype=np.float32)

        label_array = np.array(labels, dtype=np.int32)

        # ä¿å­˜ç‰¹å¾å’Œæ ‡ç­¾
        feature_path = os.path.join(self.output_dir, f"{dataset_name}_features.npy")
        label_path = os.path.join(self.output_dir, f"{dataset_name}_labels.npy")
        np.save(feature_path, feature_array)
        np.save(label_path, label_array)

        self.logger.info(f"ğŸ’¾ ä¿å­˜ {dataset_name} ç‰¹å¾åˆ° {feature_path}")
        self.logger.info(f"  - ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {feature_array.shape}")
        self.logger.info(f"  - æ ‡ç­¾çŸ©é˜µå½¢çŠ¶: {label_array.shape}")

        self.logger.info("=" * 80)
        return feature_array

    def process_all_datasets(self) -> Dict[str, Dict[str, Dict]]:
        """æ‰§è¡Œå®Œæ•´ç‰¹å¾æå–æµç¨‹

        Returns:
            Dict[str, Dict[str, Dict]]: æ‰€æœ‰æ•°æ®é›†çš„ç‰¹å¾
        """
        self.logger.info("=" * 80)
        self.logger.info("ğŸš€ å¼€å§‹ä¸‰çº§ç»“æ„ç‰¹å¾æå–æµç¨‹")

        # 1. è®­ç»ƒé˜¶æ®µï¼ˆä»…ä½¿ç”¨è®­ç»ƒæ•°æ®ï¼‰
        self.fit()

        # 2. å¤„ç†æ‰€æœ‰æ•°æ®é›†ï¼ˆåŒ…æ‹¬ç‹¬ç«‹æ•°æ®é›†ï¼‰
        all_features = {}
        for dataset in ['train_pos', 'train_neg', 'test_pos', 'test_neg', 'independent_pos', 'independent_neg']:
            try:
                all_features[dataset] = self.process_dataset(dataset)
            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†æ•°æ®é›† {dataset} å¤±è´¥: {str(e)}", exc_info=True)

        self.logger.info("ğŸ‰ ç‰¹å¾æå–å®Œæˆï¼")
        self.logger.info("=" * 80)
        return all_features


if __name__ == "__main__":
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    extractor = TertiaryStructureFeatureExtractor()

    # æ‰§è¡Œå®Œæ•´æµç¨‹
    all_features = extractor.process_all_datasets()